```

## Misc ##

### Scalable System ###

You can go through most of the examples we've seen so far in this tutorial with a simple installation of R and the Trelliscope package and its R package dependencies.  

To deal with much larger datasets, scaling comes automatically with Trelliscope's dependency on `datadr` -- any backend supported by `datadr` is supported by Trelliscope.  These currently include Hadoop and local disk.



#### Using data on localDisk as input

Here is a quick example of how to create a Trelliscope display using input data that is stored on local disk.

First, let's convert our in-memory `byLatLon` object to a "localDiskConn" object:

```{r localdisk, eval=FALSE}
# convert byLatLon to a localDiskConn object
byLatLonLD <- convert(byLatLon, 
   localDiskConn(file.path(tempdir(), "byLatLon"), autoYes=TRUE))
```

Now, we simply specify this object as the input to `makeDisplay()`:

```{r makedisplay_ld, eval=FALSE}
# make display using local disk connection as input
makeDisplay(byLatLonLD,
   name      = "co_vs_time_ld",
   group     = "co",
   desc      = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits and a cognostics function, with the data source being a local disk connection",
   panelFn   = coPanelFn,
   cogFn     = coCogFn,
   lims      = coTimeLims
)
```

The input connection is saved with the display object, and the data is used as the input when panels are rendered.  If we want to pre-render the panels, we can specify an argument `output`, which can be any `datadr` data connection.

#### Using data on HDFS as storage and Hadoop/RHIPE as compute

To illustrate creating a display with data on HDFS, we first convert `byLatLon` to an "hdfsConn" object:

```{r hdfs, eval=FALSE}
# convert byLatLon to hdfsConn
byLatLonHDFS <- convert(byLatLon, 
   hdfsConn("/tmp/byLatLon", autoYes=TRUE))
```

Since we will be pulling data at random by key from this object, we need to convert it to a Hadoop mapfile using `makeExtractable()` (`datadr` tries to make things mapfiles as much as possible, and `makeDisplay()` will check for this and let you know if your data does not comply).  

```{r makeextractable, eval=FALSE}
# make byLatLonHDFS subsets extractable by key
byLatLonHDFS <- makeExtractable(byLatLonHDFS)
```

Now, to create the display:

```{r makedisplay_hdfs, eval=FALSE}
# make display using local disk connection as input
makeDisplay(byLatLonHDFS,
   name      = "co_vs_time_hdfs",
   group     = "co",
   desc      = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits and a cognostics function, with the data source being a HDFS connection",
   panelFn   = coPanelFn,
   cogFn     = coCogFn,
   lims      = coTimeLims
)
```


### FAQ ###

#### What should I do if I have an issue or feature request?

Please post an issue on [github](https://github.com/tesseradata/trelliscope/issues).



### R Code ###

If you would like to run through all of the code examples in this documentation without having to pick out each line of code from the text, below are files with the R code for each section.  All but the final section on scalable backends should run on a workstation with no other dependencies but the required R packages.  The scalable backend code requires other components to be installed, such as Hadoop or MongoDB.

- [Getting Started, Trellis Display, VDBs](code/1intro.R)
- [Trelliscope Displays](code/2displays.R)
- [Web Notebooks](code/4webnotebook.R)
- [Scalable Backends](code/5misc.R)

