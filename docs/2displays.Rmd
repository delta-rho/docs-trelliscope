```

## Trelliscope Displays ##

### Division with datadr ###

Trelliscope is an extension of Trellis Display that allows you to create displays with potentially thousands to millions of panels.  It provides a mechanism to create and interact with these displays.

Conditioning in Trelliscope is achieved by dividing the data into subsets, one subset for each panel.  Trelliscope uses the division mechanism of the Divide & Recombine (D&R) package, `datadr`.  It is assumed that you have already gone through the `datadr` [tutorial](http://hafen.github.io/datadr).  Essentially, Trelliscope provides *visual recombination* methods for D&R.

#### Geographical division of airplane data

There are many ways we might want to split the data, depending on the purpose of our analysis.  One aspect of the `airplane` data that we are interested in is how different variables, such as `co` change over the course of a day in different geographical locations.  

Looking at our previous plots of the airplane tracks, we notice that within each square defined by the grid lines the airplane recovers many of its tracks from AM to PM:

```{r ap_tracks2, fig.height=4.5, echo=FALSE, purl=FALSE}
xyplot(latitude ~ longitude, data=airplane,
   groups=flight,
   aspect="iso",
   alpha=0.5,
   auto.key=list(space="right")
)
```

Thus, it might be interesting to split the data into subsets by cutting the latitude and longitude into small squares (okay, so these aren't really *squares*), and then study the variables of interest within those squares.  We can do this with the following:

```{r bylatlon, message=FALSE}
# latitude and longitude "square" boundaries
latCuts <- seq(38.2, 39.4, by=0.1)
lonCuts <- seq(-121.8, -120.8, by=0.1)

# divide the data by lat / lon
byLatLon <- divide(airplane, by=c("latCut", "lonCut"),
   preTransFn = function(x) {
      x$latCut <- cut(x$latitude, latCuts)
      x$lonCut <- cut(x$longitude, lonCuts)
      x
   },
   update=TRUE)
```

Note that with this small dataset, we could have appended the `latCuts` and `lonCuts` variables to the `airplane` data frame prior to calling `divide()`.

Let's look at the resulting distributed data frame ("ddf") object:

```{r}
# look at the resulting object
byLatLon
```

Also, let's look at a key-value pair to make sure it looks how we think it should:

```{r }
# see what a subset key-value pair looks like
str(byLatLon[[1]])
```

We have our division.  Now we are ready to make some displays.

### A Bare Bones Display ###

To quickly get our feet wet with creating a display, we start with a minimal example.  Creating a plot first requires the specification of what you would like to be plotted for each subset.  Just like with `lattice`, you create a panel function that will be applied to each subset.  The function is applied to each key-value pair subset in your data.  This function behaves like all other per-subset functions in `datadr`, which can operate either on both a key and a value of just the value (see [here](http://hafen.github.io/datadr/#key-value-pairs) for more details).

Some things to know about the panel function:

- The panel function is applied to each subset of your divided data object
- The panel function returns something that can be printed to a graphics device
- Those familiar with lattice can think of the panel function as the lattice panel function and the data argument(s) as the lattice packet being plotted (except that you conveniently get the whole data structure instead of just `x` and `y`)
- As long as the plotting method you use can produce graphics in a graphics device, you can use it: base R graphics, lattice, or ggplot2 are all available 
- However, using something like lattice or ggplot2 adds benefit because these create objects which can be inspected to pull out axis limits, etc. (see our discussion of `prepanel` functions later on)
- If you use something that doesn't create an object (such as base R graphics), you need to wrap the list of plot commands in an expression as the return value
- In the future, I will provide support for custom rendering hooks/methods to accommodate more plotting options

#### Panel function for `co` vs. time

Suppose we want to do a simple time series plot of the log of `co` vs. time for each geographical block, and we want to add a fitted line to the data.  Using `lattice` to create the panel function, we can do this with the following:

```{r simple_co_panel}
# a simple panel function for a trelliscope display
coPanelFn <- function(d) {
   xyplot(log2(co) ~ dat_ams, data=d, 
      type=c("p", "g"),
      panel=function(x, y, ...) {
         panel.xyplot(x, y, ...)
         try(panel.lmline(x, y, lty=2), silent=TRUE)
      })
}
```

The argument `d` (you can call it whatever you like) is the value of one of the subsets of your divided data set.  Let's test it on a subset of our data:

```{r simple_co_apply}
# apply the panel function to a subset
kvApply(coPanelFn, byLatLon[[2]])
```

#### Making the display

To create a display, applying this panel function over the entire data set, we simply call `makeDisplay()`:

```{r simple_co_display, fig.show=FALSE, message=FALSE}
# create a simple display
makeDisplay(byLatLon,
   panelFn = coPanelFn,
   name    = "co_vs_time_bb",
   group   = "co",
   desc    = "Bare bones display of co vs. time for each geographic 'square' with fitted linear model"
)
```

The two most important arguments are the first argument, `data`, and the panel function, `panelFn`.  The other arguments in this example simply identify the display, as we saw previously.  We will later see other arguments to `makeDisplay()` that provide additional useful functionality.

Here, we are putting the display in a group `"co"` since that is the variable we are currently studying.

#### Viewing the display

To view the display:

```r
# open the Trelliscope viewer for the VDB
view()
```

This will bring up the Trelliscope viewer.  You will get a list of displays to choose from (at this point, just one).  
<!--If you aren't following along with the example in your own R console, you can look at the display on RStudio's glimmer site [here](http://glimmer.rstudio.com/rhafen/vdbexample2/#group=co&name=co_vs_time_plain).-->
Later we will cover how to sync a local VDB with a web server.

You can also launch the viewer to go directly to this display with either of the following:

```r
# open this display in the Trelliscope viewer
view(group="co", name="co_vs_time_bb")
view(name="co_vs_time_bb")
```

Notice that the second one works, even though a display is uniquely identified by the group and name combination.  Usually, name is unique too and if that is the case, group is inferred.

We'll talk more about how to use the viewer later, but feel free to play around.

Now, let's do something that shows all of the other functionality of `makeDisplay()`, including prepanel functions, cognostics, etc.

### Axis Limits ###

As we discussed [before](#trellis-display), scales and axis limits are very important for creating meaningful Trellis displays.  In Trellis Display, axis limits can be computed by specifying the x and y axes as "free", "sliced", or "same".  Based on the specification, each subset is checked against what is being plotted, and the axis limits are computed.  The same can be done for Trelliscope displays.  There are different ways to do this that we will cover in this section.

In the display we just created, we see that the axis limits of the panels appear to be "free".  This is different from the `lattice` default of axis limits across panels being "same".  Since Trelliscope is very general -- any R plotting technology can potentially be used in a panel function -- the default is to not try to do anything with axis limits.

Note: the discussion in this section is constrained to two-dimensional panels (with x and y axes), which covers the vast majority of useful statistical visualization techniques.  If you have panel functions that produce plots that do not fit this (e.g. pie charts -- no!!), then the functionality described in this section is not useful.

#### Simple axis limits example

```{r simple_scales}
# setting axis limits in the call to makeDisplay()
makeDisplay(byLatLon,
   name    = "co_vs_time_same",
   group   = "co",
   desc    = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits for x and y",
   panelFn = coPanelFn,
   lims    = list(x="same", y="same")
)
```

This approach only works with `lattice` and `ggplot2` panel functions.  Per-panel axis limits are precomputed using the panel function and these are incorporated into the axis limits calculation to be applied to all panels.  The reason we are constrained to `lattice` and `ggplot2` is that Trelliscope needs to know the limits of what is being plotted in each panel to determine the overall axis limits, and these can be easily extracted from the resulting object after applying the panel function to a subset.

Here is an example of this display, using `ggplot2`:

```{r simple_scales_ggplot, message=FALSE}
# same display but using ggplot2
ggCoPanelFn <- function(x) {
   qplot(dat_ams, log2(co), data=x)
}

makeDisplay(byLatLon,
   name    = "co_vs_time_gg",
   group   = "co",
   desc    = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits for x and y, and using ggplot2",   
   panelFn = ggCoPanelFn,
   lims    = list(x="same", y="same")
)
```

Note: `ggplot2` support at the moment is pretty shaky.  For the general continuous axis scales, it should work fine, but more work needs to be done to integrate nicely.

#### Specifying a prepanel function

The previous example is the most simple way to specify axis limits.  However, it comes with a potential cost -- the panel function must be applied to each subset in order to obtain the limits.  For panel functions that take some time to render, this is wasted time.

As an alternative, we can explicitly supply a prepanel function to the `lims` argument list, called `prepanelFn`.  This functions in the same way as for `lattice`, where the prepanel function takes each subset of data and returns a list with `xlim` and `ylim`.  For example:

```{r co_display_prepanel, message=FALSE}
# using a prepanel function to compute axis limits
preFn <- function(x) {
   list(ylim=range(log2(x$co)), xlim=range(x$dat_ams))
}

makeDisplay(byLatLon,
   name    = "co_vs_time_pre",
   group   = "co",
   desc    = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits for x and y using a prepanel function",
   panelFn = coPanelFn,
   lims    = list(x="same", y="same" , prepanelFn=preFn)
)
```

#### Determining limits beforehand with `prepanel()`

In both of the above approaches, we computed axis limits at the time of creating the display.  This is not recommended for very large datasets.  There are a few reasons for this.  

1. Setting the axis limits based on "sliced" or "same" is not very robust to outliers, and we may wish to understand and modify the axis limits prior to creating the display.
2. Computing the axis limits can be more costly than creating a display, and it can be good to separate the two, particularly when we may be iterating on getting a panel function just right.

We can use a function, `prepanel()`, to compute axis limits prior to creating a display.

The main parameter to know about in `prepanel()` is `prepanelFn`, which operates in the same way as we saw before -- it is either a `lattice` or `ggplot2` panel function or it is a function that takes a subset of the data as an input and returns a list including the elements `xlim` and `ylim` (each a vector of the min and max x and y ranges of the data subset).

```{r co_prepanel, message=FALSE}
# compute axis limits prior to creating display using prepanel()
coTimePre <- prepanel(byLatLon, prepanelFn=coPanelFn)
```

#### Determining axis limits from `prepanel()` output

We can now determine our axis limits based on the results from `prepanel()`.  The output from `prepanel` is an object which has a plot method that can help in the decision of how to specify limits.

To view a plot of the panel axis limits to help in this determination:

```{r co_pre_plot}
# visualize the axis limit computations
plot(coTimePre)
```

This plot orders the axis limits for both the x and y axes for both "same" and "sliced" (with sliced ranges centered around zero).  This can help us to see if we will be squeezing the data for a lot of panels when using "same", and also helps identify outliers.  In each of the panels of this plot, you can think of the range of the "Panel Limits" axis as the range that will ultimately be chosen for each panel for the given axis and limit method.

For this plot, it appears that slicing the axis limits does not buy us much resolution, and we know that for the time variable axis (x), we would like the limits to always be the same.  Thus, we choose "same" for both axes.

To set our choice, we do the following:

```{r set_co_time_lims, message=FALSE}
# set limits from our prepanel calculations
coTimeLims <- setLims(coTimePre) # default is x="same", y="same"
```

`makeDisplay()` can take this object as is argument for `lims` and will set panel limits accordingly.

```{r co_time_lims_display, message=FALSE}
# create a display using axis limits from prepanel() and setLims()
makeDisplay(byLatLon,
   name    = "co_vs_time_lims",
   group   = "co",
   desc    = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits for x and y by pre-specifying the axis limits using prepanel() and setLims()",
   panelFn = coPanelFn,
   lims    = coTimeLims
)
```

<!-- #### Setting the limits manually

You can also explicitly specify limits with `lims` in `makeDisplay()`, which will automatically perform the axis limit computations prior to creating the display. -->

#### Setting the limits in your panel function

Another option, of course, is to set axis limits explicitly in your panel function.

<!-- This really isn't very much work for the benefit we receive from what we are able to do with the display after it's created. -->

<!-- Currently, this is how aspect ratios of the bounding boxes are specified.  There is functionality for you to specify an aspect ratio and either a height or a width, where the bounding box with which the panel aspect ratio is computed is based on the data, not the entire plot (including axis labels, etc.), but it isn't quite working.  You can specify aspect ratios in your lattice plots, but it is then difficult to get the panel dimensions correct such that there is not a lot of white space wasted for each panel (remember, we want to tile many of these across one screen so we don't want to waste white space).  On a related note, for lattice plots, see `?noMargins`. -->

### Cognostics ###

Much of the power of the viewer for multi-panel displays (particularly when the panels number in the thousands or higher) lies in the ability to specify metrics that provide interesting information about each panel, with which you can filter and sort your collection of panels to look for those which are interesting.  John Tukey called a notion similar to this "cognostics".

We can obtain cognostics for our panels by specifying a `cogFn` function to `makeDisplay()`.

The cognostics function is applied to each subset just like the panel function and must return a list which can be flattened into a data frame.  For our geographically split data, there are several cognostics we might be interested in.  Typically the most useful cognostics are arrived upon iteratively.  Here, we specify the number of observations in the subset, the slope of a fitted line to `co` vs. time, the mean latitude and longitude, and the range of the time variable.

```{r co_cog}
# create a cognostics function to be applied to each subset
coCogFn <- function(x) {
   slp <- coef(lm(co ~ as.integer(dat_ams), data=x))[2]
   if(is.na(slp))
      slp <- 0
   list(
      nobs = cog(length(which(!is.na(x$co))), desc="number of observations"),
      slope = cog(slp, desc="slope of fitted line"),
      meanLat = cogMean(x$latitude, desc="mean latitude"),
      meanLon = cogMean(x$longitude, desc="mean longitude"),
      timeRange = cogRange(as.integer(x$dat_ams[!is.na(x$co)]) / 60^2, desc="Time range (hours)")
   )
}
```

The helper functions `cog()`, `cogMean()`, `cogRange()`, etc. can be used when defining the cognostics list.  The most generic, `cog()` basically wraps the metric you want to compute with additional information, such as the description of the cognostic.  They are not necessary but are helpful.  For example, the difference between `cogRange()` and `range()` and others is that there is removal of NAs and extra checking for errors so that the cognostic calculation is robust.

Note that if you don't want to wrap your calculations in `cog()`, you don't have to, but doing so allows you to control the type of variable and give it a description.

<!-- Current types are:
- `int `: integer 
- `num `: floating point
- `fac `: factor (string)
- `date`: date
- `time`: datetime
- `geo `: geographic (a vector of lat and lon)
- `rel `: relation (not implemented)
- `hier`: hierarchy (not implemented)

If type is not specified, it is inferred based on the data being processed.

In the future, support for input variables will be added (this existed in older versions).  These will not be computed based on the data, but will be placeholders for users to provide panel-specific input. -->

Let's test the cognostics function on a subset:

```{r co_cog_apply}
# test the cognostics function on a subset
kvApply(coCogFn, byLatLon[[1]])
```

Now, let's add these cognostics to our display:

```{r set_time_lims, message=FALSE}
# add cognostics to the display
makeDisplay(byLatLon,
   name    = "co_vs_time",
   group   = "co",
   desc    = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits and a cognostics function",
   panelFn = coPanelFn,
   cogFn   = coCogFn,
   lims    = coTimeLims
)
```

<!--This display can be viewed [here](http://glimmer.rstudio.com/rhafen/vdbexample/#group=co&name=co_vs_time).-->
With this display, we can start to see the utility of cognostics.  Pressing the "cog" button in the viewer brings up a table of cognostics with ways to sort and filter the panels based on the values of the cognostics.  This is particularly useful when there are more panels than you could possibly view.

### Panel Storage ###

The default behavior for how panels are stored is to store the data subsets and then render the panels on-the-fly in the viewer.  

When we are not pre-rendering, if the input data is already a persistent divided data object, then that data object is simply used as the input when rendering is done by the viewer on-the-fly.  For example, if we have a very large data set on HDFS with a very large number of subsets, we can simply specify the HDFS connection as the input, and the only computation that needs to be done is the prepanel of cognostics computations, if specified.

When we are pre-rendering, the panels are rendered at the time `makeDisplay()` is called and the output is stored somewhere, which can be on any valid `datadr` storage backend such as local disk or HDFS.  The default is local disk in the display's VDB directory.

For the examples we have seen so far, our input has been an in-memory divided data object, which is not persistent.  By default, Trelliscope takes such input and stores it as to "localDiskConn" connection in the `"panels"` subdirectory of the display's directory in the VDB.  If we were to pre-render, the data would still go to this location, but instead of being the input data, it will be the rendered panel images.

#### When to pre-render

There are trade-offs for pre-rendering or not:

**In favor of pre-rendering**
  - Paging through panels can be faster as we don't have to render them prior to displaying them
  - Less work for the web server

**In favor of rendering on-the-fly**
  - `makeDisplay()` is considerably faster - we only need to compute cognostics and metadata about the display.  This is particularly true for very large datasets.
  - If we are only ever going to view a small subset of possibly hundreds of thousands of panels, why render them all up-front instead of rendering the ones you currently want to look at?
  - Image resolution: we do now know if the image will be viewed as a single large panel in the window, or tiled with tens or hundreds of panels in a single window.  When we render the image on-the-fly, we can render it at the resolution at which it will be viewed instead of having to store a raster image that has a resolution sufficient to be viewed at the largest scale.  A way around this would be to go away from .png to a vector format like .svg.  The only problem with that is that those files can get very large, and can be difficult to render consistently across all browsers.  In the future, this may all move toward using something like d3 as the rendering engine.

#### Pre-rendering example

Here is an example of how to specify pre-rendering when creating a display:

```{r pre_render, message=FALSE}
# make display with panels pre-rendered
makeDisplay(byLatLon,
   name      = "co_vs_time_rend",
   group     = "co",
   desc      = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits and a cognostics function, and pre-rendered panels",
   panelFn   = coPanelFn,
   cogFn     = coCogFn,
   lims      = coTimeLims,
   preRender = TRUE
)
```

The only thing that changes is that we specify `preRender=TRUE`.  Now, the data stored as the output for this display are a binary png blob for each panel.

### Cognostics Storage ###

The default behavior for storing cognostics is as an R data frame.  This provides convenience in computations regarding cognostics in the viewer, but is limiting from a scalability point of view, as it will become difficult to manage with the number of panels beyond the hundreds of thousands.  There is now experimental support for storing cognostics in a MongoDB collection.  In this case, it should be possible to handle a much larger number of panels.

To store cognostics in MongoDB, we must first initiate a cognostics MongoDB connection.  This of course requires a MongoDB instance to be running somewhere that you have access to - the following code will only work if this is the case.

```{r mongo_cog_conn, eval=FALSE}
# initiate a MongoDB cognostics connection
mongoConn <- mongoCogConn()   
```

In `mongoCogConn()`, you can specify information such as the host address, etc. of MongoDB.  The parameters and their defaults can be seen with`?mongoCogConn`.

Now, we can simply specify this as the `cogConn` argument in our call to `makeDisplay()` and cognostics will be stored in this connection.

```{r mongo_cog, eval=FALSE}
# create the display using MongoDB to store the cognostics
makeDisplay(byLatLon,
   name      = "co_vs_time_mon",
   group     = "co",
   desc      = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits and a cognostics function, and storing cognostics in MongoDB",
   panelFn   = coPanelFn,
   cogFn     = coCogFn,
   cogConn   = mongoConn,
   lims      = coTimeLims
)
```

When this display is being viewed, the MongoDB database will be queried as we interact with the panels of the display.

We chose MongoDB as a potential cognostics storage backend simply because it provides a scalable database, allows for indexing, and has a flexible aggregation framework.  The Trelliscope cognostics connection interface is extendable, such that other cognostic storage backends could be explored as well.

### Linking Displays ###

We typically have many different ways to look at the same division of data.  When creating a display against a divided dataset, Trelliscope keeps track of the division of the input data, and all displays created on the same division can be linked together in the Trelliscope viewer.

To illustrate this, here we create a display with a panel function that, for each geographical subset, simply shows all of the airplane's tracks on the grid, with the grid location of the current subset highlighted in gray.  We will see how this display can be useful when being viewed in conjunction with other displays in the [following section](#shiny-viewer).

```{r lat_lon_display, message=FALSE}
# panel function for a geographical display
latLonPanelFn <- function(a) {
   yy <- as.numeric(strsplit(gsub("\\(|\\]", "", attr(a, "split")$latCut), ",")[[1]])
   xx <- as.numeric(strsplit(gsub("\\(|\\]", "", attr(a, "split")$lonCut), ",")[[1]])
   
   xyplot(airplane$latitude ~ airplane$longitude,
      groups=airplane$dat_ams < as.POSIXct("2010-06-28 21:00:00 UTC"),
      panel=function(x, y, ...) {
         panel.rect(xx[1], yy[1], xx[2], yy[2], col="darkgray")
         panel.abline(v=lonCuts, col="gray")
         panel.abline(h=latCuts, col="gray")
         panel.xyplot(x, y, ...)
      }
   )
}

# test the panel function on a subset
kvApply(latLonPanelFn, byLatLon[[2]])
# create the display
makeDisplay(
   data = byLatLon,
   name = "lat_vs_lon",
   group = "co",
   desc = "airplane tracks",
   panelFn = latLonPanelFn
)
```

Note that the panel function uses data objects `lonCuts` and `latCuts`, data objects that are only visible in our local environment.  `makeDisplay()` checks the panel and cognostics functions to detect locally stored data and attaches it to the display object so it will be available at the time of rendering.

### Scatterplot Displays ###

Scatterplot matrices become increasingly infeasible as the number of variables grows beyond \(\approx\)10.  Using `trelliscope`, we can create "scatterplot displays", where for \(p\) variables we create a panel for each of the \(\tbinom{p}{2}\) pairwise combinations.  We can leverage research on "scagnostics" (scatterplot diagnostics, or cognostics for scatterplots) [reference][scagnostics], which includes the R package [scagnostics][rscagnostics] to identify interesting relationships.  I have created a convenience function for doing this, `splod()` (**s**catter**plo**t **d**isplay).  This function takes a data frame and creates a subset for the data for each pairwise combination of variables.  The panel function is a simple scatterplot, and the cognostics function is a collection of scagnostics.

To illustrate this, we can create a scatterplot display for all of the variables in the `airplane` data.  First, we call `makeSplodDat()` to get the data into the correct format:

```{r sploddat, message=FALSE}
# create data for scatterplot display
airplaneSplodDat <- makeSplodData(airplane, 
   id.vars=c("dat_ams", "flight", "datCut"))
```

This creates all pairwise groupings of variables not found in `id.vars` and puts them into a "ddf" object.  

Here's an example of what a subset looks like:

```{r sploddat_example}
# look at a subset of airplaneSplotDat
str(kvExample(airplaneSplodDat))
```

We can now create a display for this with `splod()`:

```{r splod, cache=TRUE, message=FALSE}
splod(airplaneSplodDat)
```

This passes `airplaneSplodDat` to `makeDisplay()`, with the default name being the same as the name of the data passed in concatenated with "_splod", a default description of "Scatterplot display", a cognostics function that computes several scagnostics (see `cogScagnostics()`), and a default plotting function that simply plots one variable vs. another.  You can override these and pass additional parameters to makeDisplay such as a custom cognostics function.


<!-- ```{r airplane_splod, cache=TRUE}
splod(airplane, id.vars=c("dat_ams", "flight", "datCut"))
``` -->


[scagnostics]: http://www.google.com/url?sa=t&rct=j&q=leland%20wilkinson%20scagnostics&source=web&cd=1&ved=0CDIQFjAA&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.62.6148%26rep%3Drep1%26type%3Dpdf&ei=6spVUcCQDYWl4AOJ_oHoDw&usg=AFQjCNH0rmXCgq5vR-mEkuYz8AC476e6Bw&sig2=VipicfJ0CN0AOJ4pt_yuXg&bvm=bv.44442042,d.dmg&cad=rja)
[rscagnostics]: http://cran.r-project.org/web/packages/scagnostics/
