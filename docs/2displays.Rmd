```

## Trelliscope Displays ##

### Division with datadr ###

Trelliscope is an extension of Trellis Display that allows you to create displays with potentially thousands to millions of panels.  It provides a mechanism to create and interact with these displays.

Conditioning in Trelliscope is achieved by dividing the data into subsets, one subset for each panel.  Trelliscope uses the division mechanism of the Divide & Recombine (D&R) package, `datadr`.  It is assumed that you have already gone through the `datadr` [tutorial](http://tesseradata.org/docs-datadr).  Essentially, Trelliscope provides *visual recombination* methods for D&R.

#### Geographical division of airplane data

There are many ways we might want to split the data, depending on the purpose of our analysis.  One aspect of the `airplane` data that we are interested in is how different variables, such as `co` change over the course of a day in different geographical locations.  

Looking at our previous plots of the airplane tracks, we notice that within each square defined by the grid lines the airplane recovers many of its tracks from AM to PM:

```{r ap_tracks2, fig.height=4.5, echo=FALSE, purl=FALSE}
xyplot(latitude ~ longitude, data=airplane,
   groups=flight,
   aspect="iso",
   alpha=0.5,
   auto.key=list(space="right")
)
```

Thus, it might be interesting to split the data into subsets by cutting the latitude and longitude into small squares (okay, so these aren't really *squares*), and then study the variables of interest within those squares.  We can do this with the following:

```{r bylatlon, message=FALSE}
# latitude and longitude "square" boundaries
latCuts <- seq(38.2, 39.4, by=0.1)
lonCuts <- seq(-121.8, -120.8, by=0.1)

# make airplane a ddf
airplaneDdf <- ddf(airplane)
# add variables latCut and lonCut to airplane
airplaneLL <- addTransform(airplaneDdf, function(x) {
   x$latCut <- cut(x$latitude, latCuts)
   x$lonCut <- cut(x$longitude, lonCuts)
   x
})
# divide the data by lat / lon
byLatLon <- divide(airplaneLL, by=c("latCut", "lonCut"),
   update=TRUE)
```

Note that with this small dataset, we could have appended the `latCuts` and `lonCuts` variables to the `airplane` data frame prior to calling `divide()`.

Let's look at the resulting distributed data frame ("ddf") object:

```{r}
# look at the resulting object
byLatLon
```

Also, let's look at a key-value pair to make sure it looks how we think it should:

```{r }
# see what a subset key-value pair looks like
byLatLon[[1]]
```

We have our division.  Now we are ready to make some displays.

### A Bare Bones Display ###

To quickly get our feet wet with creating a display, we start with a minimal example.  Creating a plot first requires the specification of what you would like to be plotted for each subset.  Just like with `lattice`, you create a panel function that will be applied to each subset.  The function is applied to each key-value pair subset in your data.  This function behaves like all other per-subset functions in `datadr`, which can operate either on both a key and a value of just the value (see [here](http://tesseradata.org/docs-datadr/#key-value-pairs) for more details).

Some things to know about the panel function:

- The panel function is applied to each subset of your divided data object
- The panel function returns something that can be printed to a graphics device or can be rendered in a web page (for example, we have experimantal support for `ggvis` and plan to support `rCharts` soon)
- Those familiar with lattice can think of the panel function as the lattice panel function and the data argument(s) as the lattice packet being plotted (except that you conveniently get the whole data structure instead of just `x` and `y`)
- Although we have been referring to lattice and have been showing examples with lattice, you do not need to use lattice in your panel function -- you can use base R graphics, lattice, or ggplot2, etc.
- However, using something like lattice or ggplot2 adds benefit because these create objects which can be inspected to pull out axis limits, etc. (see our discussion of `prepanel` functions later on)

#### Panel function for `co` vs. time

Suppose we want to do a simple time series plot of the log of `co` vs. time for each geographical block, and we want to add a fitted line to the data.  Using `lattice` to create the panel function, we can do this with the following:

```{r simple_co_panel}
# a simple panel function for a trelliscope display
coPanelFn <- function(d) {
   xyplot(log2(co) ~ dat_ams, data=d, 
      type=c("p", "g"),
      panel=function(x, y, ...) {
         panel.xyplot(x, y, ...)
         try(panel.lmline(x, y, lty=2), silent=TRUE)
      })
}
```

The argument `d` (you can call it whatever you like) is the value of one of the subsets of your divided data set.  Let's test it on a subset of our data:

```{r simple_co_apply}
# apply the panel function to a subset
coPanelFn(byLatLon[[2]]$value)
```

#### Making the display

To create a display, applying this panel function over the entire data set, we simply call `makeDisplay()`:

```{r simple_co_display, fig.show=FALSE, message=FALSE}
# create a simple display
makeDisplay(byLatLon,
   panelFn = coPanelFn,
   name    = "co_vs_time_bb",
   group   = "co",
   desc    = "Bare bones display of co vs. time for each geographic 'square' with fitted linear model"
)
```

The two most important arguments are the first argument, `data`, and the panel function, `panelFn`.  The other arguments in this example simply identify the display, as we saw previously.  We will later see other arguments to `makeDisplay()` that provide additional useful functionality.

Here, we are putting the display in a group `"co"` since that is the variable we are currently studying.

#### Viewing the display

To view the display:

```r
# open the Trelliscope viewer for the VDB
view()
```

This will bring up the Trelliscope viewer.  You will get a list of displays to choose from (at this point, just one).  
<!--If you aren't following along with the example in your own R console, you can look at the display on RStudio's glimmer site [here](http://glimmer.rstudio.com/rhafen/vdbexample2/#group=co&name=co_vs_time_plain).-->
Later we will cover how to sync a local VDB with a web server.

We'll also talk more about how to use the viewer later, but feel free to play around.

Now, let's do something that shows all of the other functionality of `makeDisplay()`, including prepanel functions, cognostics, etc.

### Axis Limits ###

As we discussed [before](#trellis-display), scales and axis limits are very important for creating meaningful Trellis displays.  In Trellis Display, axis limits can be computed by specifying the x and y axes as "free", "sliced", or "same".  Based on the specification, each subset is checked against what is being plotted, and the axis limits are computed.  The same can be done for Trelliscope displays.  There are different ways to do this that we will cover in this section.

In the display we just created, we see that the axis limits of the panels appear to be "free".  This is different from the `lattice` default of axis limits across panels being "same".  Since Trelliscope is very general -- any R plotting technology can potentially be used in a panel function -- the default is to not try to do anything with axis limits.

Note: the discussion in this section is constrained to two-dimensional panels (with x and y axes), which covers the vast majority of useful statistical visualization techniques.  If you have panel functions that produce plots that do not fit this (e.g. pie charts -- no!!), then the functionality described in this section is not useful.

#### Simple axis limits example

```{r simple_scales}
# setting axis limits in the call to makeDisplay()
makeDisplay(byLatLon,
   name    = "co_vs_time_same",
   group   = "co",
   desc    = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits for x and y",
   panelFn = coPanelFn,
   lims    = list(x="same", y="same")
)
```

This approach only works with `lattice` and `ggplot2` panel functions.  Per-panel axis limits are precomputed using the panel function and these are incorporated into the axis limits calculation to be applied to all panels.  The reason we are constrained to `lattice` and `ggplot2` is that Trelliscope needs to know the limits of what is being plotted in each panel to determine the overall axis limits, and these can be easily extracted from the resulting object after applying the panel function to a subset.

Here is an example of this display, using `ggplot2`:

```{r simple_scales_ggplot, message=FALSE}
# same display but using ggplot2
ggCoPanelFn <- function(x) {
   qplot(dat_ams, log2(co), data=x)
}

makeDisplay(byLatLon,
   name    = "co_vs_time_gg",
   group   = "co",
   desc    = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits for x and y, and using ggplot2",   
   panelFn = ggCoPanelFn,
   lims    = list(x="same", y="same")
)
```

Note: `ggplot2` support at the moment is pretty shaky.  For the general continuous axis scales, it should work fine, but more work needs to be done to integrate nicely.

#### Specifying a prepanel function

The previous example is the most simple way to specify axis limits.  However, it comes with a potential cost -- the panel function must be applied to each subset in order to obtain the limits.  For panel functions that take some time to render, this is wasted time.

As an alternative, we can explicitly supply a prepanel function to the `lims` argument list, called `prepanelFn`.  This functions in the same way as for `lattice`, where the prepanel function takes each subset of data and returns a list with `xlim` and `ylim`.  For example:

```{r co_display_prepanel, message=FALSE}
# using a prepanel function to compute axis limits
preFn <- function(x) {
   list(ylim=range(log2(x$co)), xlim=range(x$dat_ams))
}

makeDisplay(byLatLon,
   name    = "co_vs_time_pre",
   group   = "co",
   desc    = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits for x and y using a prepanel function",
   panelFn = coPanelFn,
   lims    = list(x="same", y="same" , prepanelFn=preFn)
)
```

#### Determining limits beforehand with `prepanel()`

In both of the above approaches, we computed axis limits at the time of creating the display.  This is not recommended for very large datasets.  There are a few reasons for this.  

1. Setting the axis limits based on "sliced" or "same" is not very robust to outliers, and we may wish to understand and modify the axis limits prior to creating the display.
2. Computing the axis limits can be more costly than creating a display, and it can be good to separate the two, particularly when we may be iterating on getting a panel function just right.

We can use a function, `prepanel()`, to compute axis limits prior to creating a display.

The main parameter to know about in `prepanel()` is `prepanelFn`, which operates in the same way as we saw before -- it is either a `lattice` or `ggplot2` panel function or it is a function that takes a subset of the data as an input and returns a list including the elements `xlim` and `ylim` (each a vector of the min and max x and y ranges of the data subset).

```{r co_prepanel, message=FALSE}
# compute axis limits prior to creating display using prepanel()
coTimePre <- prepanel(byLatLon, prepanelFn=coPanelFn)
```

#### Determining axis limits from `prepanel()` output

We can now determine our axis limits based on the results from `prepanel()`.  The output from `prepanel` is an object which has a plot method that can help in the decision of how to specify limits.

To view a plot of the panel axis limits to help in this determination:

```{r co_pre_plot}
# visualize the axis limit computations
plot(coTimePre)
```

This plot orders the axis limits for both the x and y axes for both "same" and "sliced" (with sliced ranges centered around zero).  This can help us to see if we will be squeezing the data for a lot of panels when using "same", and also helps identify outliers.  In each of the panels of this plot, you can think of the range of the "Panel Limits" axis as the range that will ultimately be chosen for each panel for the given axis and limit method.

For this plot, it appears that slicing the axis limits does not buy us much resolution, and we know that for the time variable axis (x), we would like the limits to always be the same.  Thus, we choose "same" for both axes.

To set our choice, we do the following:

```{r set_co_time_lims, message=FALSE}
# set limits from our prepanel calculations
coTimeLims <- setLims(coTimePre) # default is x="same", y="same"
```

`makeDisplay()` can take this object as is argument for `lims` and will set panel limits accordingly.

```{r co_time_lims_display, message=FALSE}
# create a display using axis limits from prepanel() and setLims()
makeDisplay(byLatLon,
   name    = "co_vs_time_lims",
   group   = "co",
   desc    = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits for x and y by pre-specifying the axis limits using prepanel() and setLims()",
   panelFn = coPanelFn,
   lims    = coTimeLims
)
```

#### Setting the limits in your panel function

Another option, of course, is to set axis limits explicitly in your panel function.

### Cognostics ###

Much of the power of the viewer for multi-panel displays (particularly when the panels number in the thousands or higher) lies in the ability to specify metrics that provide interesting information about each panel, with which you can filter, sort, and sample your collection of panels to look for those which are interesting.  John Tukey called a notion similar to this "cognostics".

We can obtain cognostics for our panels by specifying a `cogFn` function to `makeDisplay()`.

The cognostics function is applied to each subset just like the panel function and must return a list which can be flattened into a data frame.  For our geographically split data, there are several cognostics we might be interested in.  Typically the most useful cognostics are arrived upon iteratively.  Here, we specify the number of observations in the subset, the slope of a fitted line to `co` vs. time, the mean latitude and longitude, and the range of the time variable.

```{r co_cog}
# create a cognostics function to be applied to each subset
coCogFn <- function(x) {
   slp <- coef(lm(co ~ as.integer(dat_ams), data=x))[2]
   if(is.na(slp))
      slp <- 0
   list(
      nobs = cog(length(which(!is.na(x$co))), desc="number of observations"),
      slope = cog(slp, desc="slope of fitted line"),
      meanLat = cogMean(x$latitude, desc="mean latitude"),
      meanLon = cogMean(x$longitude, desc="mean longitude"),
      timeRange = cogRange(as.integer(x$dat_ams[!is.na(x$co)]) / 60^2, desc="Time range (hours)")
   )
}
```

The helper functions `cog()`, `cogMean()`, `cogRange()`, etc. can be used when defining the cognostics list.  The most generic, `cog()` basically wraps the metric you want to compute with additional information, such as the description of the cognostic.  They are not necessary but are helpful.  For example, the difference between `cogRange()` and `range()` and others is that there is removal of NAs and extra checking for errors so that the cognostic calculation is robust.

Note that if you don't want to wrap your calculations in `cog()`, you don't have to, but doing so allows you to control the type of variable and give it a description.

<!-- Current types are:
- `int `: integer 
- `num `: floating point
- `fac `: factor (string)
- `date`: date
- `time`: datetime
- `geo `: geographic (a vector of lat and lon)
- `rel `: relation (not implemented)
- `hier`: hierarchy (not implemented)

If type is not specified, it is inferred based on the data being processed.

In the future, support for input variables will be added (this existed in older versions).  These will not be computed based on the data, but will be placeholders for users to provide panel-specific input. -->

Let's test the cognostics function on a subset:

```{r co_cog_apply}
# test the cognostics function on a subset
coCogFn(byLatLon[[1]]$value)
```

Now, let's add these cognostics to our display:

```{r set_time_lims, message=FALSE}
# add cognostics to the display
makeDisplay(byLatLon,
   name    = "co_vs_time",
   group   = "co",
   desc    = "Plot of co vs. time for each geographic 'square' with with fitted line, illustrating the use of 'same' axis limits and a cognostics function",
   panelFn = coPanelFn,
   cogFn   = coCogFn,
   lims    = coTimeLims
)
```

<!--This display can be viewed [here](http://glimmer.rstudio.com/rhafen/vdbexample/#group=co&name=co_vs_time).-->
With this display, we can start to see the utility of cognostics.  Pressing the "cog" button in the viewer brings up a table of cognostics with ways to sort and filter the panels based on the values of the cognostics.  This is particularly useful when there are more panels than you could possibly view.

### Panel Storage ###

The default behavior for how panels are stored is to store a reference to the ddo/ddf and then render the panels on-the-fly in the viewer, pulling the appropriate subsets from the data as necessary.  Thus, if we have a very large ddo/ddf on HDFS, we do not make a copy for visualization, and only have to render the images being requested at the time of viewing.  When calling `makeDisplay()`, only the prepanel and cognostics computations need to be done.

There is an option to pre-render, which can be useful when rendering the image is compute-intensive.  However, this feature is still being developed and is currently not recommended.

### Linking Displays ###

We typically have many different ways to look at the same division of data.  When creating a display against a divided dataset, Trelliscope keeps track of the division of the input data, and all displays created on the same division can be linked together in the Trelliscope viewer.

To illustrate this, here we create a display with a panel function that, for each geographical subset, simply shows all of the airplane's tracks on the grid, with the grid location of the current subset highlighted in gray.  We will see how this display can be useful when being viewed in conjunction with other displays in the [following section](#shiny-viewer).

```{r lat_lon_display, message=FALSE}
# panel function for a geographical display
latLonPanelFn <- function(a) {
   yy <- as.numeric(strsplit(gsub("\\(|\\]", "", attr(a, "split")$latCut), ",")[[1]])
   xx <- as.numeric(strsplit(gsub("\\(|\\]", "", attr(a, "split")$lonCut), ",")[[1]])
   
   xyplot(airplane$latitude ~ airplane$longitude,
      groups=airplane$dat_ams < as.POSIXct("2010-06-28 21:00:00 UTC"),
      panel=function(x, y, ...) {
         panel.rect(xx[1], yy[1], xx[2], yy[2], col="darkgray")
         panel.abline(v=lonCuts, col="gray")
         panel.abline(h=latCuts, col="gray")
         panel.xyplot(x, y, ...)
      }
   )
}

# test the panel function on a subset
latLonPanelFn(byLatLon[[2]]$value)
# create the display
makeDisplay(
   data = byLatLon,
   name = "lat_vs_lon",
   group = "co",
   desc = "airplane tracks",
   panelFn = latLonPanelFn
)
```

Note that the panel function uses data objects `lonCuts` and `latCuts`, data objects that are only visible in our local environment.  `makeDisplay()` checks the panel and cognostics functions to detect locally stored data and attaches it to the display object so it will be available at the time of rendering.

### Scatterplot Displays ###

Scatterplot matrices become increasingly infeasible as the number of variables grows beyond \(\approx\)10.  Using `trelliscope`, we can create "scatterplot displays", where for \(p\) variables we create a panel for each of the \(\tbinom{p}{2}\) pairwise combinations.  We can leverage research on "scagnostics" (scatterplot diagnostics, or cognostics for scatterplots) [reference][scagnostics], which includes the R package [scagnostics][rscagnostics] to identify interesting relationships.  We have created a convenience function for doing this, `splod()` (**s**catter**plo**t **d**isplay).  This function takes a data frame and creates a subset for the data for each pairwise combination of variables.  The panel function is a simple scatterplot, and the cognostics function is a collection of scagnostics.

To illustrate this, we can create a scatterplot display for all of the variables in the `airplane` data.  First, we call `makeSplodDat()` to get the data into the correct format:

```{r sploddat, message=FALSE}
# create data for scatterplot display
airplaneSplodDat <- makeSplodData(airplane, 
   id.vars=c("dat_ams", "flight", "datCut"))
```

This creates all pairwise groupings of variables not found in `id.vars` and puts them into a "ddf" object.  

Here's an example of what a subset looks like:

```{r sploddat_example}
# look at a subset of airplaneSplotDat
airplaneSplodDat[[1]]
```

We can now create a display for this with `splod()`:

```{r splod, cache=TRUE, message=FALSE}
splod(airplaneSplodDat)
```

This passes `airplaneSplodDat` to `makeDisplay()`, with the default name being the same as the name of the data passed in concatenated with "_splod", a default description of "Scatterplot display", a cognostics function that computes several scagnostics (see `cogScagnostics()`), and a default plotting function that simply plots one variable vs. another.  You can override these and pass additional parameters to makeDisplay such as a custom cognostics function.


<!-- ```{r airplane_splod, cache=TRUE}
splod(airplane, id.vars=c("dat_ams", "flight", "datCut"))
``` -->


[scagnostics]: http://www.google.com/url?sa=t&rct=j&q=leland%20wilkinson%20scagnostics&source=web&cd=1&ved=0CDIQFjAA&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.62.6148%26rep%3Drep1%26type%3Dpdf&ei=6spVUcCQDYWl4AOJ_oHoDw&usg=AFQjCNH0rmXCgq5vR-mEkuYz8AC476e6Bw&sig2=VipicfJ0CN0AOJ4pt_yuXg&bvm=bv.44442042,d.dmg&cad=rja)
[rscagnostics]: http://cran.r-project.org/web/packages/scagnostics/


### Syncing Files With the Web ###

Often our D&R environment is on a local workstation which is connected to a distributed computing backend.  We might build up our VDB and web notebook locally and then desire to push the results to a web server which is running [Shiny server](http://www.rstudio.com/shiny/server/).  This is very useful for sharing analysis results with others.

There is some simple support for this in Trelliscope.  You can initialize a web connection using `webConn()`, which assumes that your web server is a Linux machine to which you have passwordless ssh capability.  You specify the address of the server, the directory of the VDB, and the name of the VDB under which you would like things stored.

```{r web_conn}
# set up a web connection
wc <- webConn(user="rhafen", ip="glimmer.rstudio.com", appDir="~/ShinyApps", name="vdbexample")
```

Now we can sync our VDB with the web:

```r
# sync files with the web server
websync()
```


